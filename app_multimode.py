import streamlit as st
import requests
import json

st.set_page_config(page_title="Multi-AI Agent", page_icon="ğŸ¤–", layout="wide")

st.markdown("""
<style>
.main-header {
    background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
    padding: 2rem;
    border-radius: 10px;
    color: white;
    text-align: center;
    margin-bottom: 2rem;
}
</style>
""", unsafe_allow_html=True)

with st.sidebar:
    st.header("âš™ï¸ Configurazione")
    groq_api_key = st.text_input("Groq API Key", type="password")
    
    st.markdown("---")
    st.markdown("""
    ### ğŸ“Š ModalitÃ 
    
    ğŸŸ¢ **QUICK** - 1 modello - 10s  
    ğŸŸ¡ **STANDARD** - 3 modelli - 30s  
    ğŸŸ  **DEEP** - 5 modelli - 60s  
    ğŸ”´ **EXPERT** - 7 modelli - 120s
    """)

st.markdown("""
<div class="main-header">
    <h1>ğŸ¤– Multi-AI Agent System</h1>
    <p>Consulta fino a 7 modelli AI</p>
</div>
""", unsafe_allow_html=True)

if not groq_api_key:
    st.warning("ğŸ‘ˆ Inserisci Groq API key")
    st.stop()

def query_groq(model, system_msg, user_msg, api_key):
    """Query Groq API directly via HTTP"""
    url = "https://api.groq.com/openai/v1/chat/completions"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg}
        ],
        "temperature": 0.7,
        "max_tokens": 1024
    }
    
    try:
        response = requests.post(url, headers=headers, json=data, timeout=30)
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        return f"Errore: {str(e)}"

st.markdown("### ğŸ’­ Fai la tua domanda")
domanda = st.text_area("", height=120, placeholder="Esempio: Dovrei cambiare lavoro?")

if domanda.strip():
    st.markdown("### âš™ï¸ Seleziona ModalitÃ ")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        quick = st.button("ğŸŸ¢ QUICK", use_container_width=True)
    with col2:
        standard = st.button("ğŸŸ¡ STANDARD", use_container_width=True)
    with col3:
        deep = st.button("ğŸŸ  DEEP", use_container_width=True)
    with col4:
        expert = st.button("ğŸ”´ EXPERT", use_container_width=True)
    
    # QUICK
    if quick:
        st.success("ğŸŸ¢ ModalitÃ  QUICK")
        with st.spinner("â³ Elaborazione..."):
            risposta = query_groq(
                "llama-3.3-70b-versatile",  # âœ… AGGIORNATO
                "Sei un esperto generalista. Fornisci risposta completa.",
                domanda,
                groq_api_key
            )
        st.markdown("### âœ… Risposta")
        st.markdown(risposta)
        st.caption("ğŸ’° Costo: $0.00 | Modello: Llama 3.3 70B")
    
    # STANDARD
    elif standard:
        st.success("ğŸŸ¡ ModalitÃ  STANDARD: 3 modelli")
        
        agents = [
            ("llama-3.1-8b-instant", "Analista Tecnico", "Analisi dettagliata"),
            ("gemma2-9b-it", "Esperto Pratico", "Esempi concreti"),
            ("llama-3.1-8b-instant", "Pensatore Critico", "Analisi critica")
        ]
        
        responses = []
        
        with st.spinner("â³ 3 agenti..."):
            for model, role, goal in agents:
                r = query_groq(model, f"Sei un {role}. {goal}.", domanda, groq_api_key)
                responses.append((role, r))
        
        # Synthesis
        with st.spinner("ğŸ¯ Sintesi..."):
            synthesis_prompt = f"Sintetizza queste 3 analisi:\n\n"
            for role, resp in responses:
                synthesis_prompt += f"{role}: {resp}\n\n"
            
            finale = query_groq(
                "llama-3.3-70b-versatile",  # âœ… AGGIORNATO
                "Sintetizza le analisi in una risposta coerente.",
                synthesis_prompt,
                groq_api_key
            )
        
        st.markdown("### âœ… Risposta Finale")
        st.markdown(finale)
        
        with st.expander("ğŸ“– Risposte individuali"):
            for role, resp in responses:
                st.markdown(f"**{role}**")
                st.info(resp)
        
        st.caption("ğŸ’° Costo: $0.00 | 3 modelli consultati")
    
    # DEEP
    elif deep:
        st.warning("ğŸŸ  ModalitÃ  DEEP: 5 modelli")
        
        agents = [
            ("llama-3.1-8b-instant", "Analista Tecnico"),
            ("llama-3.3-70b-versatile", "Stratega"),  # âœ… AGGIORNATO
            ("gemma2-9b-it", "Esperto Pratico"),
            ("llama-3.1-8b-instant", "Pensatore Critico"),
            ("llama-3.2-90b-text-preview", "Prospettiva Globale")  # âœ… CAMBIATO (qwen non sempre disponibile)
        ]
        
        responses = []
        progress = st.progress(0)
        
        for i, (model, role) in enumerate(agents):
            st.text(f"â³ {i+1}/5: {role}...")
            r = query_groq(model, f"Sei un {role}.", domanda, groq_api_key)
            responses.append((role, r))
            progress.progress((i+1)/6)
        
        st.text("ğŸ¯ Sintesi...")
        synthesis_prompt = "Sintetizza:\n\n"
        for role, resp in responses:
            synthesis_prompt += f"{role}: {resp}\n\n"
        
        finale = query_groq(
            "llama-3.3-70b-versatile",  # âœ… AGGIORNATO
            "Crea sintesi definitiva.",
            synthesis_prompt,
            groq_api_key
        )
        progress.progress(1.0)
        
        st.markdown("### âœ… Risposta DEEP")
        st.markdown(finale)
        
        with st.expander("ğŸ“Š 5 Prospettive"):
            for role, resp in responses:
                st.markdown(f"**{role}**")
                st.info(resp)
        
        st.caption("ğŸ’° Costo: $0.00 | 5 modelli consultati")
    
    # EXPERT
    elif expert:
        st.error("ğŸ”´ ModalitÃ  EXPERT: 6 modelli")
        
        agents = [
            ("llama-3.1-8b-instant", "Analista Veloce"),
            ("llama-3.3-70b-versatile", "Stratega Master"),  # âœ… AGGIORNATO
            ("deepseek-r1-distill-llama-70b", "Pensatore Profondo"),  # âœ… AGGIORNATO
            ("gemma2-9b-it", "Esperto Pratico"),
            ("llama-3.1-8b-instant", "Critico Costruttivo"),
            ("llama-3.2-90b-text-preview", "Verificatore Globale")
        ]
        
        responses = []
        progress = st.progress(0)
        
        for i, (model, role) in enumerate(agents):
            st.text(f"â³ {i+1}/6: {role}...")
            r = query_groq(model, f"Sei un {role}.", domanda, groq_api_key)
            responses.append((role, r))
            progress.progress((i+1)/7)
        
        st.text("ğŸ¯ Super-sintesi...")
        synthesis_prompt = "Sintesi da 6 AI:\n\n"
        for role, resp in responses:
            synthesis_prompt += f"{role}: {resp}\n\n"
        
        finale = query_groq(
            "llama-3.3-70b-versatile",  # âœ… AGGIORNATO
            "Sintesi definitiva master.",
            synthesis_prompt,
            groq_api_key
        )
        progress.progress(1.0)
        
        st.markdown("### ğŸ† Risposta EXPERT")
        st.markdown(finale)
        
        with st.expander("ğŸ“Š 6 Prospettive"):
            for role, resp in responses:
                st.markdown(f"**{role}**")
                st.info(resp)
        
        st.caption("ğŸ’° Costo: $0.00 | 6 modelli premium consultati")

st.markdown("---")
st.markdown("**Multi-AI System** | Powered by Groq API | Modelli: Llama 3.3, Mixtral, Gemma")
